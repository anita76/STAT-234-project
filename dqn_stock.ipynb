{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-LLC/ElegantRL/blob/master/eRL_demo_StockTrading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1gUG3OCJ5GS"
   },
   "source": [
    "# **Stock Trading Application in ElegantRL**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGXyBBvL0dR2"
   },
   "source": [
    "# **Part 1: Problem Formulation**\n",
    "Formally, we model stock trading as a Markov Decision Process (MDP), and formulate the trading objective as maximization of expected return:\n",
    "\n",
    "\n",
    "\n",
    "*   **State s = [b, p, h]**: a vector that includes the remaining balance b, stock prices p, and stock shares h. p and h are vectors with D dimension, where D denotes the number of stocks. \n",
    "*   **Action a**: a vector of actions over D stocks. The allowed actions on each stock include selling, buying, or holding, which result in decreasing, increasing, or no change of the stock shares in h, respectively.\n",
    "*   **Reward r(s, a, s’)**: The asset value change of taking action a at state s and arriving at new state s’.\n",
    "*   **Policy π(s)**: The trading strategy at state s, which is a probability distribution of actions at state s.\n",
    "*   **Q-function Q(s, a)**: the expected return (reward) of taking action a at state s following policy π.\n",
    "*   **State-transition**: After taking the actions a, the number of shares h is modified, as shown in Fig 3, and the new portfolio is the summation of the balance and the total value of the stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jESlu_WtD_JS"
   },
   "source": [
    "# **Part 2: Stock Trading Environment Design**\n",
    "\n",
    "**State Space and Action Space**\n",
    "\n",
    "\n",
    "*   **State Space**: We use a 181-dimensional vector consists of seven parts of information to represent the state space of multiple stocks trading environment: [b, p, h, M, R, C, X], where b is the balance, p is the stock prices, h is the number of shares, M is the Moving Average Convergence Divergence (MACD), R is the Relative Strength Index (RSI), C is the Commodity Channel Index (CCI), and X is the Average Directional Index (ADX).\n",
    "*   **Action Space**: As a recap, we have three types of actions: selling, buying, and holding for a single stock. We use the negative value for selling, positive value for buying, and zero for holding. In this case, the action space is defined as {-k, …, -1, 0, 1, …, k}, where k is the maximum share to buy or sell in each transaction.\n",
    "\n",
    "\n",
    "**Easy-to-customize Features**\n",
    "\n",
    "\n",
    "*   **initial_capital**: the initial capital that the user wants to invest.\n",
    "*   **tickers**: the stocks that the user wants to trade with.\n",
    "*   **initial_stocks**: the initial amount of each stock and the default could be zero.\n",
    "*   **buy_cost_pct, sell_cost_pct**: the transaction fee of each buying or selling transaction.\n",
    "*   **max_stock**: the user is able to define the maximum number of stocks that are allowed to trade per transaction. Users can also set the maximum percentage of capitals to invest in each stock.\n",
    "*   **tech_indicator_list**: the list of financial indicators that are taken into account, which is used to define a state.\n",
    "*   **start_date, start_eval_date, end_eval_date**: the training and backtesting time intervals. Thee time dates (or timestamps) are used, once the training period is specified, the rest is backtesting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbamGVHC3AeW"
   },
   "source": [
    "# **Part 3: Install ElegantRL and related packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVdmpnK_3Zcn"
   },
   "source": [
    "# **Part 4: Import Packages**\n",
    "\n",
    "\n",
    "*   **ElegantRL**\n",
    "*   **yfinance**: yfinance aims to solve this problem by offering a reliable, threaded, and Pythonic way to download historical market data from Yahoo! finance.\n",
    "*   **StockDataFrame**: stockstats inherits and extends pandas.DataFrame to support Stock Statistics and Stock Indicators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1VM1xKujoz-6"
   },
   "outputs": [],
   "source": [
    "from run import *\n",
    "from agent import AgentDQN, AgentUADQN\n",
    "from StockTrading import StockTradingEnv, check_stock_trading_env\n",
    "import yfinance as yf\n",
    "from stockstats import StockDataFrame as Sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n8zcgcn14uq"
   },
   "source": [
    "# **Part 5: Specify Agent and Environment**\n",
    "\n",
    "*   **args.agent**: firstly chooses one DRL algorithm to use from agent.py. In this application, we prefer to choose DDPG and PPO agent.\n",
    "*   **args.env**: creates the environment, and the user can either customize own environment or preprocess environments from OpenAI Gym and PyBullet Gym from env.py. In this application, we create the self-designed stock trading environment.\n",
    "\n",
    "\n",
    "> Before finishing initialization of **args**, please see Arguments() in run.py for more details about adjustable hyper-parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E03f6cTeajK4",
    "outputId": "2e47cb16-baa0-487d-ca71-ac6c2d249e6b"
   },
   "outputs": [],
   "source": [
    "# Agent\n",
    "args = Arguments(if_on_policy=False)\n",
    "args.agent = AgentUADQN()\n",
    "\n",
    "# Environment\n",
    "tickers = 'TARA'  # finrl.config.NAS_74_TICKER\n",
    "\n",
    "tech_indicator_list = [\n",
    "  'macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30',\n",
    "  'close_30_sma', 'close_60_sma']  # finrl.config.TECHNICAL_INDICATORS_LIST\n",
    "\n",
    "gamma = 0.99\n",
    "max_stock = 1\n",
    "initial_capital = 1e6\n",
    "initial_stocks = 100\n",
    "buy_cost_pct = 1e-3\n",
    "sell_cost_pct = 1e-3\n",
    "start_date = '2008-03-19'\n",
    "start_eval_date = '2016-01-01'\n",
    "end_eval_date = '2021-01-01'\n",
    "\n",
    "args.env = StockTradingEnv('./', gamma, max_stock, initial_capital, buy_cost_pct, \n",
    "                           sell_cost_pct, start_date, start_eval_date, \n",
    "                           end_eval_date, tickers, tech_indicator_list, \n",
    "                           initial_stocks, if_eval=False, if_save=True)\n",
    "args.env_eval = StockTradingEnv('./', gamma, max_stock, initial_capital, buy_cost_pct, \n",
    "                           sell_cost_pct, start_date, start_eval_date, \n",
    "                           end_eval_date, tickers, tech_indicator_list, \n",
    "                           initial_stocks, if_eval=True, if_save=True)\n",
    "\n",
    "args.env.target_reward = 3\n",
    "args.env_eval.target_reward = 3\n",
    "\n",
    "# Hyperparameters\n",
    "args.gamma = gamma\n",
    "args.break_step = 2000\n",
    "args.net_dim = 2 ** 9\n",
    "args.max_step = args.env.max_step\n",
    "args.max_memo = 2000\n",
    "args.batch_size = 32\n",
    "args.repeat_times = 4 # repeat_times * target_step == number of times we update before training\n",
    "args.eval_gap = 2 ** 4\n",
    "args.eval_times1 = 2 ** 3\n",
    "args.eval_times2 = 2 ** 5\n",
    "args.if_allow_break = True\n",
    "args.rollout_num = 2 # the number of rollout workers (larger is not always faster)\n",
    "args.target_step = 50 # number of exploration steps before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1j5kLHF2dhJ"
   },
   "source": [
    "# **Part 6: Train and Evaluate the Agent**\n",
    "\n",
    "> The training and evaluating processes are all finished inside function **train_and_evaluate_mp()**, and the only parameter for it is **args**. It includes the fundamental objects in DRL:\n",
    "\n",
    "*   agent,\n",
    "*   environment.\n",
    "\n",
    "> And it also includes the parameters for training-control:\n",
    "\n",
    "*   batch_size,\n",
    "*   target_step,\n",
    "*   reward_scale,\n",
    "*   gamma, etc.\n",
    "\n",
    "> The parameters for evaluation-control:\n",
    "\n",
    "*   break_step,\n",
    "*   random_seed, etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KGOPSD6da23k",
    "outputId": "2fdbfc4b-80a2-4659-ace5-b3bde0f36d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| GPU id: 0, cwd: ./AgentUADQN/StockTradingEnv-v1_0\n",
      "| Remove history\n",
      "ID      Step      MaxR |    avgR      stdR       avgQ      avgL |  avgS  stdS\n",
      "start exploring before training\n",
      "Begin updating network before training\n",
      "start training\n",
      "explore environment\n",
      "update environment: total_steps=100\n",
      "0   5.00e+01      0.92 |\n",
      "0   5.00e+01      0.92 |0.921527  0.000000   0.054135  1.684125 |  1258     0\n",
      "explore environment\n",
      "update environment: total_steps=150\n",
      "0   1.00e+02      0.92 |0.860943  0.000000   0.093488  2.444844 |  1258     0\n",
      "explore environment\n",
      "update environment: total_steps=200\n",
      "explore environment\n",
      "update environment: total_steps=250\n",
      "explore environment\n",
      "update environment: total_steps=300\n",
      "explore environment\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-28f454bd012d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the training process will terminate once it reaches the target reward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/jupyteruser/31484685/STAT-234-project/run.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    161\u001b[0m                or os.path.exists(f'{cwd}/stop')):\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'explore environment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mtotal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update environment: total_steps={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jupyteruser/31484685/STAT-234-project/agent.py\u001b[0m in \u001b[0;36mexplore_env\u001b[0;34m(self, env, buffer, target_step, reward_scale, gamma)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexplore_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfti\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_fti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/share/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jupyteruser/31484685/STAT-234-project/agent.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, fti)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfti\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for discrete action space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mnet1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mnet1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_quantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args) # the training process will terminate once it reaches the target reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPXOxLSqh5cP"
   },
   "source": [
    "Understanding the above results::\n",
    "*   **Step**: the total training steps.\n",
    "*  **MaxR**: the maximum reward.\n",
    "*   **avgR**: the average of the rewards.\n",
    "*   **stdR**: the standard deviation of the rewards.\n",
    "*   **objA**: the objective function value of Actor Network (Policy Network).\n",
    "*   **objC**: the objective function value (Q-value)  of Critic Network (Value Network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6jvgYPnHMpf"
   },
   "source": [
    "# **Part 7: Backtest and Draw the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.env_eval.draw_cumulative_return(args, torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3jCdezgzQUhh",
    "outputId": "a1ea5861-1ce9-4d44-c94d-3c236eb48428"
   },
   "outputs": [],
   "source": [
    "args.env_eval.draw_cumulative_return_while_learning(args, torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "eRL_demo_StockTrading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
